# MedSpa AI Reconciliation Engine v2.0.0

ğŸš€ **Advanced ML-powered transaction reconciliation for medical spas**

This engine automatically matches Alle Rewards, Aspire certificates, and POS transactions with 90%+ accuracy using state-of-the-art machine learning and NLP techniques.

## ğŸ¯ Features

### Core Capabilities
- **Intelligent Transaction Matching**: ML-powered matching with multiple similarity algorithms
- **Real-time Processing**: Async batch processing with progress tracking
- **Advanced NLP**: Name normalization, semantic similarity, and fuzzy matching
- **Comprehensive Scoring**: Multi-factor confidence scoring with detailed breakdowns
- **Performance Monitoring**: Real-time system health and performance metrics
- **Export Capabilities**: JSON and CSV export with detailed results

### Technical Features
- **Multi-threaded Processing**: Parallel batch processing for high throughput
- **Model Persistence**: Automatic model saving and loading
- **Error Handling**: Robust error handling with detailed logging
- **API-First Design**: RESTful API with comprehensive documentation
- **Health Monitoring**: System resource monitoring and alerting

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Server    â”‚    â”‚ Reconciliation   â”‚    â”‚ Confidence      â”‚
â”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚    Engine        â”‚â—„â”€â”€â–ºâ”‚   Scorer        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Job Manager   â”‚    â”‚   Batch Processorâ”‚    â”‚   ML Models     â”‚
â”‚   (Async)       â”‚    â”‚   (ThreadPool)   â”‚    â”‚   (NLP/ML)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- 4GB+ RAM (for NLP models)
- 2GB+ disk space

### Installation

1. **Clone and navigate to the AI directory:**
```bash
cd python-ai
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Install spaCy model:**
```bash
python -m spacy download en_core_web_sm
```

4. **Run the API server:**
```bash
python api_server.py
```

The server will start on `http://localhost:8000` with automatic API documentation at `http://localhost:8000/docs`.

### Test the Engine

Run the comprehensive test suite:
```bash
python test_engine.py
```

## ğŸ“Š API Endpoints

### Health & Status
- `GET /health` - System health check
- `GET /status` - Comprehensive system status
- `GET /model/info` - Model information
- `GET /model/metrics` - Performance metrics

### Single Predictions
- `POST /predict` - Predict match for single transaction pair
- `POST /predict/batch` - Batch prediction for multiple pairs

### Job Management
- `POST /reconcile/start` - Start async reconciliation job
- `GET /reconcile/jobs` - List active jobs
- `GET /reconcile/jobs/{job_id}` - Get job status
- `GET /reconcile/jobs/{job_id}/results` - Get job results
- `DELETE /reconcile/jobs/{job_id}` - Cancel job
- `GET /reconcile/history` - Job history

### File Upload
- `POST /upload/reward-transactions` - Upload reward CSV
- `POST /upload/pos-transactions` - Upload POS CSV

### Model Training
- `POST /model/train` - Retrain ML model

### Export & Analytics
- `POST /export` - Export results (JSON/CSV)
- `GET /analytics/performance` - Performance analytics
- `GET /analytics/jobs/{job_id}` - Job analytics

## ğŸ’¡ Usage Examples

### 1. Single Transaction Prediction

```python
import requests

# Predict match between two transactions
response = requests.post("http://localhost:8000/predict", json={
    "reward_transaction": {
        "customer_name": "Sarah Johnson",
        "customer_phone": "(555) 123-4567",
        "amount": 450.00,
        "date": "2024-01-15",
        "service": "Botox Treatment"
    },
    "pos_transaction": {
        "customer_name": "Sarah Johnson",
        "customer_phone": "(555) 123-4567",
        "amount": 450.00,
        "date": "2024-01-15",
        "service": "Botox Treatment"
    },
    "threshold": 0.95
})

print(response.json())
# Output:
# {
#   "match_probability": 0.98,
#   "confidence_level": "Very High",
#   "recommendation": "AUTO_APPROVE",
#   "processing_time_ms": 45,
#   "result": "match"
# }
```

### 2. Batch Reconciliation

```python
import requests
import time

# Start reconciliation job
job_response = requests.post("http://localhost:8000/reconcile/start", json={
    "reward_transactions": [
        {
            "customer_name": "Sarah Johnson",
            "amount": 450.00,
            "date": "2024-01-15"
        }
    ],
    "pos_transactions": [
        {
            "customer_name": "Sarah Johnson",
            "amount": 450.00,
            "date": "2024-01-15"
        }
    ],
    "threshold": 0.8
})

job_id = job_response.json()["job_id"]

# Monitor progress
while True:
    status = requests.get(f"http://localhost:8000/reconcile/jobs/{job_id}").json()
    print(f"Progress: {status['progress_percent']:.1f}%")
    
    if status['status'] == 'completed':
        break
    time.sleep(1)

# Get results
results = requests.get(f"http://localhost:8000/reconcile/jobs/{job_id}/results").json()
print(f"Matches found: {results['summary']['matches_found']}")
```

### 3. File Upload and Processing

```python
import requests

# Upload reward transactions
with open('reward_transactions.csv', 'rb') as f:
    reward_response = requests.post(
        "http://localhost:8000/upload/reward-transactions",
        files={"file": f}
    )

# Upload POS transactions
with open('pos_transactions.csv', 'rb') as f:
    pos_response = requests.post(
        "http://localhost:8000/upload/pos-transactions",
        files={"file": f}
    )

# Start reconciliation with uploaded data
reconciliation_response = requests.post("http://localhost:8000/reconcile/start", json={
    "reward_transactions": reward_response.json()["transactions"],
    "pos_transactions": pos_response.json()["transactions"],
    "threshold": 0.85
})
```

## ğŸ”§ Configuration

### Environment Variables
```bash
# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# Engine Configuration
MAX_WORKERS=4
BATCH_SIZE=100
MODEL_THRESHOLD=0.95

# Performance Configuration
MAX_MEMORY_GB=4
CPU_LIMIT_PERCENT=90
```

### Model Configuration
The engine uses multiple ML models:
- **spaCy**: Text processing and semantic similarity
- **Sentence Transformers**: Advanced semantic matching
- **Random Forest**: Classification for match prediction
- **Fuzzy Matching**: Name and text similarity

## ğŸ“ˆ Performance Metrics

### Typical Performance
- **Processing Speed**: 100-500 transactions/second
- **Accuracy**: 90-95% for well-formatted data
- **Memory Usage**: 2-4GB for large datasets
- **Response Time**: <100ms for single predictions

### Scaling Considerations
- **Horizontal Scaling**: Multiple API instances
- **Vertical Scaling**: Increase workers and batch size
- **Database Integration**: Add persistent storage
- **Caching**: Redis for frequent queries

## ğŸ§ª Testing

### Run Test Suite
```bash
python test_engine.py
```

### Test Coverage
- âœ… Confidence scoring algorithms
- âœ… Single transaction prediction
- âœ… Batch reconciliation jobs
- âœ… System health monitoring
- âœ… Model metrics and performance
- âœ… Export functionality
- âœ… Error handling and edge cases

### Sample Data
The test suite includes realistic sample data with:
- Exact matches
- Name variations (Dr. vs no title)
- Amount differences
- Date variations
- Non-matching transactions

## ğŸ” Troubleshooting

### Common Issues

**1. spaCy Model Not Found**
```bash
python -m spacy download en_core_web_sm
```

**2. Memory Issues**
- Reduce `MAX_WORKERS` and `BATCH_SIZE`
- Increase system RAM
- Monitor with `/health` endpoint

**3. Slow Performance**
- Check CPU usage with `/analytics/performance`
- Optimize batch size for your data
- Consider model retraining

**4. Low Accuracy**
- Review data quality
- Retrain model with more data
- Adjust threshold settings

### Debug Mode
Enable detailed logging:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ”„ Model Training

### Retrain with New Data
```python
import requests

training_data = [
    {
        "reward_transaction": {...},
        "pos_transaction": {...},
        "is_match": True
    }
]

response = requests.post("http://localhost:8000/model/train", json={
    "training_data": training_data,
    "validation_split": 0.2
})

print(f"Training accuracy: {response.json()['accuracy']}")
```

### Training Data Format
- Minimum 10 samples required
- Balanced positive/negative examples
- Real-world transaction variations
- Include edge cases and errors

## ğŸ“Š Monitoring & Analytics

### System Health
```bash
curl http://localhost:8000/health
```

### Performance Analytics
```bash
curl http://localhost:8000/analytics/performance
```

### Job Analytics
```bash
curl http://localhost:8000/analytics/jobs/{job_id}
```

## ğŸš€ Production Deployment

### Docker Deployment
```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
RUN python -m spacy download en_core_web_sm

COPY . .
EXPOSE 8000

CMD ["python", "api_server.py"]
```

### Environment Setup
```bash
# Production requirements
pip install gunicorn
gunicorn api_server:app -w 4 -k uvicorn.workers.UvicornWorker
```

### Monitoring
- **Health Checks**: `/health` endpoint
- **Metrics**: `/analytics/performance`
- **Logging**: Structured JSON logging
- **Alerting**: CPU/Memory thresholds

## ğŸ¤ Contributing

### Development Setup
1. Fork the repository
2. Create feature branch
3. Add tests for new functionality
4. Run test suite
5. Submit pull request

### Code Standards
- Follow PEP 8
- Add type hints
- Include docstrings
- Write comprehensive tests

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For support and questions:
- Check the troubleshooting section
- Review API documentation at `/docs`
- Run the test suite for validation
- Monitor system health endpoints

## ğŸ”„ Disaster Recovery & Backup Management

### Overview
The MedSpaSync Pro ecosystem includes a comprehensive disaster recovery system that provides automated backups, configuration management, rollback mechanisms, and recovery testing capabilities.

### Features
- **Automated Backups**: Daily scheduled backups at 2:00 AM
- **Multiple Backup Types**: Full system, database-only, and configuration-only backups
- **Compression**: Automatic compression to save storage space
- **Integrity Verification**: SHA256 checksums for backup validation
- **Retention Management**: Configurable retention policies (default: 30 days)
- **Restoration**: One-click restore from any backup
- **Testing**: Comprehensive test suite for validation

### Backup Types

#### Full System Backup
```bash
# Create full system backup
curl -X POST "http://localhost:8000/api/v1/backup/create" \
  -H "Content-Type: application/json" \
  -d '{"type": "full"}'
```

#### Database Backup
```bash
# Create database-only backup
curl -X POST "http://localhost:8000/api/v1/backup/create" \
  -H "Content-Type: application/json" \
  -d '{"type": "database"}'
```

#### Configuration Backup
```bash
# Create configuration-only backup
curl -X POST "http://localhost:8000/api/v1/backup/create" \
  -H "Content-Type: application/json" \
  -d '{"type": "config"}'
```

### Backup Management

#### Check Backup Status
```bash
# Get backup system status
curl "http://localhost:8000/api/v1/backup/status"
```

#### List Recent Backups
```bash
# List recent backups (default: 10)
curl "http://localhost:8000/api/v1/backup/list?limit=20"
```

#### Verify Backup Integrity
```bash
# Verify backup integrity
curl "http://localhost:8000/api/v1/backup/verify/backup_20241201_143022_abc123"
```

### Restoration Procedures

#### Full System Restore
```bash
# Restore entire system from backup
curl -X POST "http://localhost:8000/api/v1/backup/restore/backup_20241201_143022_abc123" \
  -H "Content-Type: application/json" \
  -d '{"type": "all"}'
```

#### Selective Restore
```bash
# Restore only database
curl -X POST "http://localhost:8000/api/v1/backup/restore/backup_20241201_143022_abc123" \
  -H "Content-Type: application/json" \
  -d '{"type": "database"}'

# Restore only configurations
curl -X POST "http://localhost:8000/api/v1/backup/restore/backup_20241201_143022_abc123" \
  -H "Content-Type: application/json" \
  -d '{"type": "config"}'
```

### Ecosystem Commands

#### Create Backup
```bash
# Create system backup
./start_ecosystem.sh backup
```

#### Check Backup Status
```bash
# Check backup system status
./start_ecosystem.sh backup-status
```

#### List Backups
```bash
# List recent backups
./start_ecosystem.sh backup-list

# List specific number of backups
./start_ecosystem.sh backup-list 20
```

#### Restore from Backup
```bash
# Restore from backup (with confirmation)
./start_ecosystem.sh restore backup_20241201_143022_abc123

# Restore specific type
./start_ecosystem.sh restore backup_20241201_143022_abc123 database
```

#### Verify Backup
```bash
# Verify backup integrity
./start_ecosystem.sh verify backup_20241201_143022_abc123
```

#### Run Disaster Recovery Tests
```bash
# Run comprehensive disaster recovery tests
./start_ecosystem.sh test-dr
```

### Configuration

#### Backup Configuration
```python
from disaster_recovery import BackupConfig

config = BackupConfig(
    backup_dir="backups",
    retention_days=30,
    max_backups=100,
    compression=True,
    include_logs=True,
    include_configs=True,
    include_database=True,
    backup_schedule="0 2 * * *",  # Daily at 2 AM
    verify_backups=True
)
```

#### Environment Variables
```bash
# Database configuration for backups
DB_HOST=localhost
DB_PORT=5432
DB_NAME=medspasync
DB_USER=postgres
DB_PASSWORD=your_password

# Backup storage
BACKUP_DIR=backups
BACKUP_RETENTION_DAYS=30
BACKUP_MAX_COUNT=100
```

### Testing

#### Run Disaster Recovery Tests
```bash
# Run comprehensive test suite
cd python-ai
python test_disaster_recovery.py
```

#### Test Coverage
- âœ… Backup creation (full, database, config)
- âœ… Backup verification and integrity checks
- âœ… Backup restoration procedures
- âœ… Backup cleanup and retention policies
- âœ… Error handling and edge cases
- âœ… Performance testing
- âœ… API endpoint validation

#### Manual Testing
```bash
# 1. Create test backup
./start_ecosystem.sh backup

# 2. Verify backup
./start_ecosystem.sh verify <backup_id>

# 3. Test restoration (in test environment)
./start_ecosystem.sh restore <backup_id>

# 4. Verify system functionality
./start_ecosystem.sh status
```

### Monitoring

#### Backup System Health
```bash
# Check backup system status
curl "http://localhost:8000/api/v1/backup/status"
```

#### Metrics Integration
The disaster recovery system integrates with the Prometheus metrics stack:
- Backup success/failure rates
- Backup size and duration metrics
- Disk usage monitoring
- Retention policy compliance

#### Alerting
Configure alerts for:
- Backup failures
- Disk space warnings (>80% usage)
- Retention policy violations
- Restoration attempts

### Best Practices

#### Backup Strategy
1. **Daily Full Backups**: Automated at 2:00 AM
2. **Weekly Verification**: Manual verification of recent backups
3. **Monthly Testing**: Full restore testing in staging environment
4. **Quarterly Review**: Review and update retention policies

#### Storage Management
1. **Monitor Disk Usage**: Keep below 80% capacity
2. **Compression**: Enable compression to save space
3. **Retention Policies**: Balance storage costs with recovery needs
4. **Offsite Storage**: Consider cloud backup for critical data

#### Security
1. **Access Control**: Restrict backup access to authorized personnel
2. **Encryption**: Consider encrypting sensitive backup data
3. **Audit Logging**: Monitor backup and restore activities
4. **Testing**: Regular testing of restore procedures

#### Recovery Procedures
1. **Documentation**: Maintain detailed recovery procedures
2. **Training**: Train team members on recovery processes
3. **Testing**: Regular testing of recovery procedures
4. **Communication**: Plan for communication during recovery

### Troubleshooting

#### Common Issues

**1. Backup Creation Fails**
```bash
# Check disk space
df -h

# Check database connectivity
pg_isready -h localhost -p 5432

# Check backup directory permissions
ls -la backups/
```

**2. Backup Verification Fails**
```bash
# Check backup file integrity
sha256sum backups/database/backup_*.sql.gz

# Verify backup metadata
cat backups/metadata/backups.json
```

**3. Restoration Fails**
```bash
# Check database connectivity
pg_isready -h localhost -p 5432

# Verify backup exists
ls -la backups/database/backup_*.sql.gz

# Check restoration logs
tail -f logs/backup.log
```

**4. Disk Space Issues**
```bash
# Check disk usage
df -h

# Clean up old backups
./start_ecosystem.sh backup-status

# Adjust retention policy
# Edit disaster_recovery.py configuration
```

### Recovery Scenarios

#### Complete System Failure
1. **Assess Damage**: Determine extent of failure
2. **Choose Recovery Point**: Select appropriate backup
3. **Restore Infrastructure**: Restore database and configurations
4. **Verify Functionality**: Test all critical systems
5. **Document Incident**: Record lessons learned

#### Database Corruption
1. **Stop Services**: Prevent further corruption
2. **Identify Last Good Backup**: Find most recent valid backup
3. **Restore Database**: Restore from backup
4. **Verify Data Integrity**: Check data consistency
5. **Restart Services**: Resume normal operations

#### Configuration Issues
1. **Identify Problem**: Determine configuration issue
2. **Restore Configurations**: Restore from backup
3. **Verify Settings**: Check all configuration parameters
4. **Test Functionality**: Ensure system works correctly
5. **Update Documentation**: Record configuration changes

### Integration with Monitoring

The disaster recovery system integrates with the existing monitoring stack:

#### Prometheus Metrics
- `backup_success_total`: Total successful backups
- `backup_failure_total`: Total failed backups
- `backup_duration_seconds`: Backup duration
- `backup_size_bytes`: Backup size
- `disk_usage_percent`: Disk usage percentage

#### Grafana Dashboards
- Backup success/failure rates
- Backup size trends
- Disk usage monitoring
- Recovery time objectives

#### Alerting Rules
```yaml
# Backup failure alert
- alert: BackupFailure
  expr: backup_failure_total > 0
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Backup failure detected"
    description: "System backup has failed"

# Disk space alert
- alert: HighDiskUsage
  expr: disk_usage_percent > 80
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "High disk usage detected"
    description: "Backup disk usage is above 80%"
```

## Frontend Performance Monitoring

The MedSpaSync Pro ecosystem includes a comprehensive frontend performance monitoring system that tracks user experience metrics, errors, and performance indicators in real-time.

### Features

- **Web Vitals Tracking**: Captures Core Web Vitals (CLS, FID, FCP, LCP, TTFB)
- **Error Monitoring**: Tracks JavaScript errors and promise rejections
- **User Interaction Analytics**: Monitors clicks, form submissions, and navigation
- **Page Load Performance**: Measures page load times and resource loading
- **Real-time Metrics**: Buffered metrics sent to backend every 30 seconds
- **Session Analytics**: Tracks user sessions and performance patterns
- **Alert System**: Automatic alerts for performance degradation
- **Dashboard Visualization**: Real-time performance dashboard for admins

### Architecture

```
Frontend (React) â†’ Performance Reporter â†’ Backend API â†’ Database â†’ Dashboard
```

### Frontend Implementation

#### Performance Reporter Utility

Located at `src/utils/performanceReporter.js`, this utility provides:

- **Automatic Initialization**: Starts monitoring when the app loads
- **Web Vitals Integration**: Uses the `web-vitals` library
- **Error Boundary Support**: Integrates with React error boundaries
- **Session Management**: Generates unique session IDs
- **Batch Processing**: Buffers metrics for efficient transmission

#### Key Components

1. **PerformanceReporter Class**: Main monitoring class
2. **Web Vitals Tracking**: Core Web Vitals measurement
3. **Error Tracking**: JavaScript and promise error capture
4. **Interaction Tracking**: User interaction monitoring
5. **Navigation Tracking**: SPA navigation monitoring

#### Usage

```javascript
import { initPerformanceReporter, recordMetric } from './utils/performanceReporter';

// Initialize (done automatically in main.jsx)
const reporter = initPerformanceReporter({
  apiEndpoint: '/api/v1/monitoring/frontend-metrics',
  environment: 'production',
  enableRealTime: true,
  enableErrorTracking: true,
  enableUserInteractions: true
});

// Record custom metrics
recordMetric('custom', 'feature_usage', 100, 'ms', {
  feature: 'reconciliation',
  action: 'upload'
});
```

### Backend API Endpoints

#### POST `/api/v1/monitoring/frontend-metrics`
Receives frontend performance metrics from the client.

**Request Body:**
```json
{
  "metrics": [
    {
      "timestamp": "2024-01-01T12:00:00Z",
      "session_id": "session_123",
      "user_id": "user_456",
      "metric_type": "web_vital",
      "metric_name": "LCP",
      "value": 2500,
      "unit": "ms",
      "metadata": {
        "rating": "good",
        "delta": 100
      }
    }
  ],
  "session_id": "session_123",
  "user_id": "user_456",
  "environment": "production"
}
```

#### GET `/api/v1/monitoring/frontend-summary`
Returns performance summary statistics.

**Response:**
```json
{
  "status": "success",
  "data": {
    "active_sessions": 15,
    "avg_load_time": 1200,
    "error_rate": 0.02,
    "total_metrics": 1250,
    "alerts_count": 3
  }
}
```

#### GET `/api/v1/monitoring/frontend-alerts`
Returns active performance alerts.

#### GET `/api/v1/monitoring/frontend-session/{session_id}`
Returns analytics for a specific session.

#### GET `/api/v1/monitoring/frontend-page/{page_url}`
Returns performance analytics for a specific page.

### Dashboard Access

#### Performance Dashboard
- **URL**: `/performance`
- **Access**: Admin users only
- **Features**: Real-time metrics, charts, alerts, session analytics

#### Test Page
- **URL**: `/performance-test`
- **Access**: Admin users only
- **Features**: Manual testing of performance monitoring

### Metrics Types

#### Web Vitals
- **CLS** (Cumulative Layout Shift): Measures visual stability
- **FID** (First Input Delay): Measures interactivity
- **FCP** (First Contentful Paint): Measures perceived load speed
- **LCP** (Largest Contentful Paint): Measures loading performance
- **TTFB** (Time to First Byte): Measures server response time

#### Custom Metrics
- **Page Load Times**: Total page load duration
- **Resource Load Times**: Individual resource loading performance
- **Error Counts**: JavaScript errors and promise rejections
- **User Interactions**: Clicks, form submissions, navigation
- **Session Data**: User session information and patterns

### Alert System

The system automatically generates alerts for:

- **High Error Rates**: >5% error rate triggers warning
- **Slow Page Loads**: >3s load time triggers alert
- **Poor Web Vitals**: Below "good" threshold triggers alert
- **High Resource Usage**: Slow resource loading triggers alert

### Configuration

#### Frontend Configuration
```javascript
const config = {
  apiEndpoint: '/api/v1/monitoring/frontend-metrics',
  environment: 'production',
  batchSize: 10,
  flushInterval: 30000, // 30 seconds
  enableRealTime: true,
  enableErrorTracking: true,
  enableUserInteractions: true
};
```

#### Backend Configuration
```python
# In frontend_monitoring.py
ALERT_THRESHOLDS = {
    'error_rate': 0.05,  # 5%
    'load_time': 3000,   # 3 seconds
    'cls': 0.1,          # 0.1
    'fid': 100,          # 100ms
    'lcp': 2500          # 2.5 seconds
}
```

### Testing

#### Manual Testing
1. Navigate to `/performance-test`
2. Run performance tests
3. Simulate errors
4. Check metrics in dashboard

#### Automated Testing
```bash
# Test frontend monitoring
npm test -- --testPathPattern=performanceReporter

# Test backend monitoring
python -m pytest tests/test_frontend_monitoring.py
```

### Troubleshooting

#### Common Issues

1. **Metrics Not Sending**
   - Check network connectivity
   - Verify API endpoint is accessible
   - Check browser console for errors

2. **High Error Rates**
   - Review JavaScript errors in console
   - Check for unhandled promise rejections
   - Verify error boundary implementation

3. **Slow Performance**
   - Monitor resource loading times
   - Check for large bundle sizes
   - Review third-party script impact

#### Debug Mode
Enable debug mode in development:
```javascript
if (import.meta.env.DEV) {
  window.performanceReporter = performanceReporter;
}
```

### Performance Impact

The monitoring system is designed to have minimal impact on application performance:

- **Lightweight**: <1KB gzipped
- **Asynchronous**: Non-blocking metric collection
- **Batched**: Efficient network usage
- **Conditional**: Can be disabled in production if needed

### Security Considerations

- **Data Privacy**: No PII collected without consent
- **Rate Limiting**: Backend implements rate limiting
- **Authentication**: Admin-only dashboard access
- **Data Retention**: Configurable data retention policies

### Integration with Existing Systems

The frontend performance monitoring integrates with:

- **Prometheus/Grafana**: Metrics exported to Prometheus
- **Alerting System**: Integrated with existing alert infrastructure
- **Logging**: Performance data logged for analysis
- **Analytics**: Session data available for business intelligence

### Future Enhancements

- **Real-time WebSocket Updates**: Live dashboard updates
- **A/B Testing Integration**: Performance comparison between variants
- **User Journey Tracking**: Complete user flow analysis
- **Predictive Analytics**: Performance trend prediction
- **Mobile App Support**: React Native integration

---

**MedSpa AI Reconciliation Engine v2.0.0** - Powering the future of medical spa transaction reconciliation with advanced AI technology. 